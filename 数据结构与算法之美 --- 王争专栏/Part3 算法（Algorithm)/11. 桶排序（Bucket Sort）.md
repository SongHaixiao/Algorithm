# 桶排序（Bucket Sort）

> 桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。

- 例子：对这组金额在 0 - 50 之间的订单进行排序

  <img src="../Resources/26.jpg" style="zoom:50%;" />

  ## 时间复杂度分析

  如果要排序的数据有 n 个，把它们均匀地划分到 m 个桶内，每个桶里就有 $k=n/m$ 个元素。

  每个桶**内部使用快速排序**，时间复杂度为 $O(k * logk)$。

  m 个桶排序的时间复杂度就是 $O(m * k * logk)$，因为 $k=n/m$，所以整个桶排序的时间复杂度就是 $O(n*log(n/m))$。

  当桶的个数 $m$ 接近数据个数 $n$ 时，$log(n/m)$ 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 $O(n)$。

  ## 桶排序是不是可以替代之前讲的排序算法呢？

  答案是否定的。**桶排序对要排序数据的要求是非常苛刻的。**

  首先，**要排序的数据需要很容易就能划分成 m 个桶**，并且，**桶与桶之间有着天然的大小顺序**。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。

  其次，**数据在各个桶之间的分布是比较均匀的**。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 $O(nlogn)$ 的排序算法了。

  ## 桶排序比较适合用在**外部排序**中。

  所谓的**外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。**

  比如说有 10GB 的订单数据，希望按订单金额（假设金额都是正整数）进行排序，但是内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？

  可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后得到，订单金额最小是 1 元，最大是 10 万元。将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02…99）。

  理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。

  不过，订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该怎么办呢？

  针对这些划分之后还是比较大的文件，可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元…901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。





*`@ 笔记时间 ：2020-8-23	FROM	极客时间 《算法啊与数据结构之美》 王争  专栏`* 